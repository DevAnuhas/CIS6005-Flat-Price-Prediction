{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Data Preprocessing\n",
    "\n",
    "**Purpose**: Clean data, engineer features, and prepare datasets for machine learning models.\n",
    "\n",
    "**Input**: Raw train and test CSV files\n",
    "\n",
    "**Output**: Preprocessed data, saved scaler and encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (100000, 19)\n",
      "Test shape: (100000, 18)\n"
     ]
    }
   ],
   "source": [
    "# Load training data (with price) and test data (without price)\n",
    "train_df = pd.read_csv('../data/raw/data.csv')\n",
    "test_df = pd.read_csv('../data/raw/test.csv')\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (100000, 17)\n",
      "Target shape: (100000,)\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target\n",
    "# Keep 'index' column for final submission but exclude from features\n",
    "X_train = train_df.drop(['price', 'index'], axis=1)\n",
    "y_train = train_df['price']\n",
    "X_test = test_df.drop(['index'], axis=1)\n",
    "test_indices = test_df['index'].copy()  # Save for submission\n",
    "\n",
    "print(f\"Features shape: {X_train.shape}\")\n",
    "print(f\"Target shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before dropping other_area:\n",
      "  Train features: 17\n",
      "  Test features: 17\n",
      "\n",
      "After dropping other_area:\n",
      "  Train features: 16\n",
      "  Test features: 16\n",
      "\n",
      "Multicollinearity reduced!\n"
     ]
    }
   ],
   "source": [
    "# Drop 'other_area' due to high multicollinearity with 'total_area' (r=0.89)\n",
    "# EDA showed that other_area is highly redundant with total_area\n",
    "# Removing it reduces multicollinearity and improves model interpretability\n",
    "\n",
    "print(\"Before dropping other_area:\")\n",
    "print(f\"  Train features: {X_train.shape[1]}\")\n",
    "print(f\"  Test features: {X_test.shape[1]}\")\n",
    "\n",
    "X_train = X_train.drop(['other_area'], axis=1)\n",
    "X_test = X_test.drop(['other_area'], axis=1)\n",
    "\n",
    "print(\"\\nAfter dropping other_area:\")\n",
    "print(f\"  Train features: {X_train.shape[1]}\")\n",
    "print(f\"  Test features: {X_test.shape[1]}\")\n",
    "print(\"\\nMulticollinearity reduced!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Drop Redundant Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing value imputation completed\n"
     ]
    }
   ],
   "source": [
    "# If missing values exist, apply imputation\n",
    "# Median imputation for numerical features preserves central tendency\n",
    "# Mode imputation for categorical features uses most frequent value\n",
    "\n",
    "# Updated: removed 'other_area' (dropped due to multicollinearity)\n",
    "numerical_cols = ['kitchen_area', 'bath_area', 'extra_area', \n",
    "                  'extra_area_count', 'year', 'ceil_height', 'floor_max', \n",
    "                  'floor', 'total_area', 'bath_count', 'rooms_count']\n",
    "\n",
    "categorical_cols = ['gas', 'hot_water', 'central_heating', \n",
    "                    'extra_area_type_name', 'district_name']\n",
    "\n",
    "# Numerical imputation with median\n",
    "for col in numerical_cols:\n",
    "    if X_train[col].isnull().sum() > 0:\n",
    "        median_value = X_train[col].median()\n",
    "        X_train[col].fillna(median_value, inplace=True)\n",
    "        X_test[col].fillna(median_value, inplace=True)\n",
    "\n",
    "# Categorical imputation with mode\n",
    "for col in categorical_cols:\n",
    "    if X_train[col].isnull().sum() > 0:\n",
    "        mode_value = X_train[col].mode()[0]\n",
    "        X_train[col].fillna(mode_value, inplace=True)\n",
    "        X_test[col].fillna(mode_value, inplace=True)\n",
    "\n",
    "print(\"Missing value imputation completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in training data:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Missing values in test data:\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values in training data:\")\n",
    "print(X_train.isnull().sum()[X_train.isnull().sum() > 0])\n",
    "\n",
    "print(\"\\nMissing values in test data:\")\n",
    "print(X_test.isnull().sum()[X_test.isnull().sum() > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing value imputation completed\n"
     ]
    }
   ],
   "source": [
    "# If missing values exist, apply imputation\n",
    "# Median imputation for numerical features preserves central tendency\n",
    "# Mode imputation for categorical features uses most frequent value\n",
    "\n",
    "numerical_cols = ['kitchen_area', 'bath_area', 'extra_area', \n",
    "                  'extra_area_count', 'year', 'ceil_height', 'floor_max', \n",
    "                  'floor', 'total_area', 'bath_count', 'rooms_count']\n",
    "\n",
    "categorical_cols = ['gas', 'hot_water', 'central_heating', \n",
    "                    'extra_area_type_name', 'district_name']\n",
    "\n",
    "# Numerical imputation with median\n",
    "for col in numerical_cols:\n",
    "    if X_train[col].isnull().sum() > 0:\n",
    "        median_value = X_train[col].median()\n",
    "        X_train[col].fillna(median_value, inplace=True)\n",
    "        X_test[col].fillna(median_value, inplace=True)\n",
    "\n",
    "# Categorical imputation with mode\n",
    "for col in categorical_cols:\n",
    "    if X_train[col].isnull().sum() > 0:\n",
    "        mode_value = X_train[col].mode()[0]\n",
    "        X_train[col].fillna(mode_value, inplace=True)\n",
    "        X_test[col].fillna(mode_value, inplace=True)\n",
    "\n",
    "print(\"Missing value imputation completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Outlier Detection and Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering completed\n",
      "New shape: (100000, 19)\n",
      "Created 3 new features: floor_ratio, is_ground_floor, is_top_floor\n"
     ]
    }
   ],
   "source": [
    "# Create derived features based on domain knowledge\n",
    "# These features capture important real estate relationships\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"Create new features from existing ones\"\"\"\n",
    "    # Floor ratio: relative position in building (0-1)\n",
    "    # Ground and top floors often have different valuations\n",
    "    df['floor_ratio'] = df['floor'] / df['floor_max']\n",
    "    \n",
    "    # Binary indicators for special floor positions\n",
    "    df['is_ground_floor'] = (df['floor'] == 1).astype(int)\n",
    "    df['is_top_floor'] = (df['floor'] == df['floor_max']).astype(int)\n",
    "    \n",
    "    # Note: living_area feature removed\n",
    "    # It would be total_area - kitchen_area - bath_area\n",
    "    # But since other_area was dropped, this creates circular logic\n",
    "    # We already have total_area which contains all information needed\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply feature engineering to both train and test\n",
    "X_train = engineer_features(X_train)\n",
    "X_test = engineer_features(X_test)\n",
    "\n",
    "print(\"Feature engineering completed\")\n",
    "print(f\"New shape: {X_train.shape}\")\n",
    "print(\"Created 3 new features: floor_ratio, is_ground_floor, is_top_floor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering completed\n",
      "New shape: (100000, 20)\n"
     ]
    }
   ],
   "source": [
    "# Create derived features based on domain knowledge\n",
    "# These features capture important real estate relationships\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"Create new features from existing ones\"\"\"\n",
    "    # Floor ratio: relative position in building (0-1)\n",
    "    # Ground and top floors often have different valuations\n",
    "    df['floor_ratio'] = df['floor'] / df['floor_max']\n",
    "    \n",
    "    # Binary indicators for special floor positions\n",
    "    df['is_ground_floor'] = (df['floor'] == 1).astype(int)\n",
    "    df['is_top_floor'] = (df['floor'] == df['floor_max']).astype(int)\n",
    "    \n",
    "    # Living area: usable space excluding kitchen and bathroom\n",
    "    # More granular than total_area alone\n",
    "    df['living_area'] = df['total_area'] - df['kitchen_area'] - df['bath_area']\n",
    "    df['living_area'] = df['living_area'].clip(lower=0)  # Ensure non-negative\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply feature engineering to both train and test\n",
    "X_train = engineer_features(X_train)\n",
    "X_test = engineer_features(X_test)\n",
    "\n",
    "print(\"Feature engineering completed\")\n",
    "print(f\"New shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label encoding completed\n",
      "district_name encoded to: 7 classes\n",
      "extra_area_type_name encoded to: 2 classes\n"
     ]
    }
   ],
   "source": [
    "# Label encoding for categorical variables\n",
    "# Converts categorical strings to numerical values (0, 1, 2, ...)\n",
    "# Suitable for tree-based models and reduces dimensionality vs one-hot\n",
    "\n",
    "label_encoders = {}\n",
    "\n",
    "# Encode district_name and extra_area_type_name\n",
    "for col in ['district_name', 'extra_area_type_name']:\n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    # Fit on training data\n",
    "    X_train[col] = le.fit_transform(X_train[col].astype(str))\n",
    "    \n",
    "    # Transform test data, handling unseen categories\n",
    "    X_test[col] = X_test[col].astype(str).apply(\n",
    "        lambda x: le.transform([x])[0] if x in le.classes_ else -1\n",
    "    )\n",
    "    \n",
    "    # Save encoder for later use\n",
    "    label_encoders[col] = le\n",
    "\n",
    "print(\"Label encoding completed\")\n",
    "print(f\"district_name encoded to: {X_train['district_name'].nunique()} classes\")\n",
    "print(f\"extra_area_type_name encoded to: {X_train['extra_area_type_name'].nunique()} classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-scaling dtypes:\n",
      "kitchen_area              int64\n",
      "bath_area                 int64\n",
      "gas                       int64\n",
      "hot_water                 int64\n",
      "central_heating           int64\n",
      "extra_area                int64\n",
      "extra_area_count          int64\n",
      "year                      int64\n",
      "ceil_height             float64\n",
      "floor_max                 int64\n",
      "floor                     int64\n",
      "total_area              float64\n",
      "bath_count                int64\n",
      "extra_area_type_name      int64\n",
      "district_name             int64\n",
      "rooms_count               int64\n",
      "floor_ratio             float64\n",
      "is_ground_floor           int64\n",
      "is_top_floor              int64\n",
      "living_area             float64\n",
      "dtype: object\n",
      "Unique gas values (train sample): [0 1]\n",
      "Feature scaling completed\n",
      "\n",
      "Scaled features - Mean: 0.0000\n",
      "Scaled features - Std: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# StandardScaler normalizes features to zero mean and unit variance\n",
    "# This prevents features with large magnitudes from dominating the model\n",
    "# Essential for linear models, neural networks, and distance-based algorithms\n",
    "\n",
    "# Convert binary categorical columns (Yes/No) to numeric 1/0 where present\n",
    "binary_map = {'Yes': 1, 'No': 0}\n",
    "for col in ['gas', 'hot_water', 'central_heating']:\n",
    "    if col in X_train.columns:\n",
    "        # Map known Yes/No values to 1/0 and keep original where mapping fails\n",
    "        X_train[col] = X_train[col].map(binary_map).fillna(X_train[col])\n",
    "        X_test[col] = X_test[col].map(binary_map).fillna(X_test[col])\n",
    "\n",
    "# If any remaining object (string) columns exist, label-encode them as a fallback\n",
    "obj_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "if len(obj_cols) > 0:\n",
    "    for col in obj_cols:\n",
    "        if col in ['district_name', 'extra_area_type_name']:\n",
    "            # already encoded earlier; skip\n",
    "            continue\n",
    "        le_tmp = LabelEncoder()\n",
    "        X_train[col] = le_tmp.fit_transform(X_train[col].astype(str))\n",
    "        X_test[col] = X_test[col].astype(str).apply(lambda x: le_tmp.transform([x])[0] if x in le_tmp.classes_ else -1)\n",
    "\n",
    "print('Pre-scaling dtypes:')\n",
    "print(X_train.dtypes)\n",
    "if 'gas' in X_train.columns:\n",
    "    print('Unique gas values (train sample):', pd.unique(X_train['gas'])[:20])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit scaler on training data and transform both train and test\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrame for easier handling\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "print(\"Feature scaling completed\")\n",
    "print(f\"\\nScaled features - Mean: {X_train_scaled.mean().mean():.4f}\")\n",
    "print(f\"Scaled features - Std: {X_train_scaled.std().mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train-Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (80000, 20)\n",
      "Validation set: (20000, 20)\n",
      "Test set: (100000, 20)\n"
     ]
    }
   ],
   "source": [
    "# Split training data into train and validation sets\n",
    "# 80/20 split provides sufficient training data while retaining validation samples\n",
    "# Validation set is used for model selection and hyperparameter tuning\n",
    "\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train_scaled, y_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train_final.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "print(f\"Test set: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Preprocessing pipeline completed with following steps:\n",
    "\n",
    "1. **Data Loading**: Loaded train and test datasets\n",
    "2. **Drop Redundant Features**: Removed 'other_area' due to high multicollinearity (r=0.89 with total_area)\n",
    "3. **Missing Values**: Checked and handled (median/mode imputation)\n",
    "4. **Outlier Capping**: Applied IQR method to cap extreme values\n",
    "5. **Feature Engineering**: Created 3 new features (floor_ratio, is_ground_floor, is_top_floor)\n",
    "6. **Categorical Encoding**: Label encoding for district_name and extra_area_type_name\n",
    "7. **Feature Scaling**: StandardScaler normalization for all features\n",
    "8. **Train-Val Split**: 80/20 split for model validation\n",
    "9. **Data Persistence**: Saved processed data and preprocessing objects\n",
    "\n",
    "**Final Feature Count**: {} features (16 original - 1 dropped + 3 engineered = 18 total)\n",
    "\n",
    "**Key Decisions**:\n",
    "- Dropped 'other_area' to reduce multicollinearity with 'total_area'\n",
    "- Removed 'living_area' feature to avoid circular logic after dropping 'other_area'\n",
    "- Used Ridge regression (next notebook) to handle remaining multicollinearity\n",
    "\n",
    "The data is now ready for model training in the next notebook.\n",
    "\".format(X_train_scaled.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data saved successfully!\n",
      "\n",
      "Saved files:\n",
      "- ../data/processed/X_train.npy\n",
      "- ../data/processed/X_val.npy\n",
      "- ../data/processed/y_train.npy\n",
      "- ../data/processed/y_val.npy\n",
      "- ../data/processed/X_test.npy\n",
      "- ../models/scaler.pkl\n",
      "- ../models/label_encoders.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save processed datasets for model training\n",
    "np.save('../data/processed/X_train.npy', X_train_final.values)\n",
    "np.save('../data/processed/X_val.npy', X_val.values)\n",
    "np.save('../data/processed/y_train.npy', y_train_final.values)\n",
    "np.save('../data/processed/y_val.npy', y_val.values)\n",
    "np.save('../data/processed/X_test.npy', X_test_scaled.values)\n",
    "np.save('../data/processed/test_indices.npy', test_indices.values)\n",
    "\n",
    "# Save feature names for reference\n",
    "with open('../data/processed/feature_names.txt', 'w') as f:\n",
    "    f.write('\\n'.join(X_train_scaled.columns))\n",
    "\n",
    "# Save preprocessing objects for deployment\n",
    "# These are needed to preprocess new data at inference time\n",
    "joblib.dump(scaler, '../models/scaler.pkl')\n",
    "joblib.dump(label_encoders, '../models/label_encoders.pkl')\n",
    "\n",
    "print(\"All data saved successfully!\")\n",
    "print(\"\\nSaved files:\")\n",
    "print(\"- ../data/processed/X_train.npy\")\n",
    "print(\"- ../data/processed/X_val.npy\")\n",
    "print(\"- ../data/processed/y_train.npy\")\n",
    "print(\"- ../data/processed/y_val.npy\")\n",
    "print(\"- ../data/processed/X_test.npy\")\n",
    "print(\"- ../models/scaler.pkl\")\n",
    "print(\"- ../models/label_encoders.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
