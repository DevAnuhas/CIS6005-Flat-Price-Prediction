{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Data Preprocessing\n",
    "\n",
    "This notebook prepares the dataset for model training. We clean the data, handle outliers, engineer new features, encode categorical variables, and scale the features before splitting into train and validation sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Libraries and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T11:30:30.286919Z",
     "iopub.status.busy": "2026-02-07T11:30:30.286463Z",
     "iopub.status.idle": "2026-02-07T11:30:31.135334Z",
     "shell.execute_reply": "2026-02-07T11:30:31.134814Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T11:30:31.136657Z",
     "iopub.status.busy": "2026-02-07T11:30:31.136524Z",
     "iopub.status.idle": "2026-02-07T11:30:31.279077Z",
     "shell.execute_reply": "2026-02-07T11:30:31.278580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (100000, 19)\n",
      "Test shape: (100000, 18)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('../data/raw/data.csv')\n",
    "test_df = pd.read_csv('../data/raw/test.csv')\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T11:30:31.296864Z",
     "iopub.status.busy": "2026-02-07T11:30:31.296732Z",
     "iopub.status.idle": "2026-02-07T11:30:31.305148Z",
     "shell.execute_reply": "2026-02-07T11:30:31.304703Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (100000, 17)\n",
      "Target shape: (100000,)\n"
     ]
    }
   ],
   "source": [
    "X_train = train_df.drop(['price', 'index'], axis=1)\n",
    "y_train = train_df['price']\n",
    "X_test = test_df.drop(['index'], axis=1)\n",
    "test_indices = test_df['index'].copy()\n",
    "\n",
    "print(f\"Features shape: {X_train.shape}\")\n",
    "print(f\"Target shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop `other_area` because our EDA revealed a very high correlation with `total_area` (r = 0.89). Keeping both would introduce multicollinearity without adding useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T11:30:31.306269Z",
     "iopub.status.busy": "2026-02-07T11:30:31.306193Z",
     "iopub.status.idle": "2026-02-07T11:30:31.315453Z",
     "shell.execute_reply": "2026-02-07T11:30:31.315009Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features after drop: 16\n"
     ]
    }
   ],
   "source": [
    "# Drop other_area (r=0.89 with total_area, identified in EDA)\n",
    "X_train = X_train.drop(['other_area'], axis=1)\n",
    "X_test = X_test.drop(['other_area'], axis=1)\n",
    "\n",
    "print(f\"Features after drop: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Missing Value Handling\n",
    "\n",
    "We use median imputation for numerical features and mode imputation for categorical features. Although our dataset has no missing values, we include this step as a safeguard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T11:30:31.316644Z",
     "iopub.status.busy": "2026-02-07T11:30:31.316585Z",
     "iopub.status.idle": "2026-02-07T11:30:31.342462Z",
     "shell.execute_reply": "2026-02-07T11:30:31.342068Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in train: 0\n",
      "Missing values in test:  0\n"
     ]
    }
   ],
   "source": [
    "numerical_cols = ['kitchen_area', 'bath_area', 'extra_area', \n",
    "                  'extra_area_count', 'year', 'ceil_height', 'floor_max', \n",
    "                  'floor', 'total_area', 'bath_count', 'rooms_count']\n",
    "\n",
    "categorical_cols = ['gas', 'hot_water', 'central_heating', \n",
    "                    'extra_area_type_name', 'district_name']\n",
    "\n",
    "for col in numerical_cols:\n",
    "    if X_train[col].isnull().sum() > 0:\n",
    "        median_value = X_train[col].median()\n",
    "        X_train[col].fillna(median_value, inplace=True)\n",
    "        X_test[col].fillna(median_value, inplace=True)\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if X_train[col].isnull().sum() > 0:\n",
    "        mode_value = X_train[col].mode()[0]\n",
    "        X_train[col].fillna(mode_value, inplace=True)\n",
    "        X_test[col].fillna(mode_value, inplace=True)\n",
    "\n",
    "print(f\"Missing values in train: {X_train.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in test:  {X_test.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Outlier Handling\n",
    "\n",
    "We apply IQR-based capping to continuous features. Values below Q1 - 1.5 * IQR or above Q3 + 1.5 * IQR are clipped to those bounds. We fit the bounds on the training set and apply them to both train and test to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T11:30:31.343644Z",
     "iopub.status.busy": "2026-02-07T11:30:31.343577Z",
     "iopub.status.idle": "2026-02-07T11:30:31.358152Z",
     "shell.execute_reply": "2026-02-07T11:30:31.357732Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IQR outlier capping applied to:\n",
      "  kitchen_area: [-4.00, 36.00]\n",
      "  bath_area: [-8.50, 51.50]\n",
      "  extra_area: [-10.00, 30.00]\n",
      "  ceil_height: [0.61, 5.65]\n",
      "  total_area: [12.29, 121.26]\n"
     ]
    }
   ],
   "source": [
    "continuous_cols = ['kitchen_area', 'bath_area', 'extra_area', 'ceil_height', 'total_area']\n",
    "\n",
    "outlier_bounds = {}\n",
    "\n",
    "for col in continuous_cols:\n",
    "    Q1 = X_train[col].quantile(0.25)\n",
    "    Q3 = X_train[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    outlier_bounds[col] = (lower, upper)\n",
    "\n",
    "    X_train[col] = X_train[col].clip(lower=lower, upper=upper)\n",
    "    X_test[col] = X_test[col].clip(lower=lower, upper=upper)\n",
    "\n",
    "print(\"IQR outlier capping applied to:\")\n",
    "for col, (lo, hi) in outlier_bounds.items():\n",
    "    print(f\"  {col}: [{lo:.2f}, {hi:.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "We create four derived features based on domain knowledge of the property market: floor position ratio, ground and top floor indicators, and usable living area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T11:30:31.359142Z",
     "iopub.status.busy": "2026-02-07T11:30:31.359082Z",
     "iopub.status.idle": "2026-02-07T11:30:31.362995Z",
     "shell.execute_reply": "2026-02-07T11:30:31.362713Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features after engineering: 20\n",
      "New features: floor_ratio, is_ground_floor, is_top_floor, living_area\n"
     ]
    }
   ],
   "source": [
    "def engineer_features(df):\n",
    "    df['floor_ratio'] = df['floor'] / df['floor_max']\n",
    "    df['is_ground_floor'] = (df['floor'] == 1).astype(int)\n",
    "    df['is_top_floor'] = (df['floor'] == df['floor_max']).astype(int)\n",
    "    df['living_area'] = (df['total_area'] - df['kitchen_area'] - df['bath_area']).clip(lower=0)\n",
    "    return df\n",
    "\n",
    "X_train = engineer_features(X_train)\n",
    "X_test = engineer_features(X_test)\n",
    "\n",
    "print(f\"Features after engineering: {X_train.shape[1]}\")\n",
    "print(f\"New features: floor_ratio, is_ground_floor, is_top_floor, living_area\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Categorical Encoding\n",
    "\n",
    "We use label encoding for `district_name` and `extra_area_type_name`, and map the binary Yes/No columns to 1/0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T11:30:31.364183Z",
     "iopub.status.busy": "2026-02-07T11:30:31.364126Z",
     "iopub.status.idle": "2026-02-07T11:30:34.671849Z",
     "shell.execute_reply": "2026-02-07T11:30:34.671423Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "district_name: 7 classes\n",
      "extra_area_type_name: 2 classes\n"
     ]
    }
   ],
   "source": [
    "label_encoders = {}\n",
    "\n",
    "for col in ['district_name', 'extra_area_type_name']:\n",
    "    le = LabelEncoder()\n",
    "    X_train[col] = le.fit_transform(X_train[col].astype(str))\n",
    "    X_test[col] = X_test[col].astype(str).apply(\n",
    "        lambda x: le.transform([x])[0] if x in le.classes_ else -1\n",
    "    )\n",
    "    label_encoders[col] = le\n",
    "\n",
    "print(f\"district_name: {X_train['district_name'].nunique()} classes\")\n",
    "print(f\"extra_area_type_name: {X_train['extra_area_type_name'].nunique()} classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T11:30:34.673026Z",
     "iopub.status.busy": "2026-02-07T11:30:34.672964Z",
     "iopub.status.idle": "2026-02-07T11:30:34.689136Z",
     "shell.execute_reply": "2026-02-07T11:30:34.688720Z"
    }
   },
   "outputs": [],
   "source": [
    "# Map binary Yes/No columns to 1/0\n",
    "binary_map = {'Yes': 1, 'No': 0}\n",
    "for col in ['gas', 'hot_water', 'central_heating']:\n",
    "    X_train[col] = X_train[col].map(binary_map).fillna(X_train[col])\n",
    "    X_test[col] = X_test[col].map(binary_map).fillna(X_test[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Scaling\n",
    "\n",
    "We apply StandardScaler to normalise all features to zero mean and unit variance. The scaler is fitted on the training set only to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T11:30:34.690472Z",
     "iopub.status.busy": "2026-02-07T11:30:34.690397Z",
     "iopub.status.idle": "2026-02-07T11:30:34.713797Z",
     "shell.execute_reply": "2026-02-07T11:30:34.713379Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled features - Mean: 0.0000\n",
      "Scaled features - Std:  1.0000\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "print(f\"Scaled features - Mean: {X_train_scaled.mean().mean():.4f}\")\n",
    "print(f\"Scaled features - Std:  {X_train_scaled.std().mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train-Validation Split\n",
    "\n",
    "We split the training data 80/20. The validation set is used to evaluate and compare models without touching the Kaggle test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T11:30:34.714917Z",
     "iopub.status.busy": "2026-02-07T11:30:34.714855Z",
     "iopub.status.idle": "2026-02-07T11:30:34.722401Z",
     "shell.execute_reply": "2026-02-07T11:30:34.722080Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:   (80000, 20)\n",
      "Validation set: (20000, 20)\n",
      "Test set:       (100000, 20)\n"
     ]
    }
   ],
   "source": [
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train_scaled, y_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set:   {X_train_final.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "print(f\"Test set:       {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Processed Data\n",
    "\n",
    "We save the processed arrays and preprocessing objects so the modelling notebooks can load them directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T11:30:34.723583Z",
     "iopub.status.busy": "2026-02-07T11:30:34.723523Z",
     "iopub.status.idle": "2026-02-07T11:30:34.737365Z",
     "shell.execute_reply": "2026-02-07T11:30:34.736904Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data and preprocessing objects saved.\n"
     ]
    }
   ],
   "source": [
    "np.save('../data/processed/X_train.npy', X_train_final.values)\n",
    "np.save('../data/processed/X_val.npy', X_val.values)\n",
    "np.save('../data/processed/y_train.npy', y_train_final.values)\n",
    "np.save('../data/processed/y_val.npy', y_val.values)\n",
    "np.save('../data/processed/X_test.npy', X_test_scaled.values)\n",
    "np.save('../data/processed/test_indices.npy', test_indices.values)\n",
    "\n",
    "with open('../data/processed/feature_names.txt', 'w') as f:\n",
    "    f.write('\\n'.join(X_train_scaled.columns))\n",
    "\n",
    "joblib.dump(scaler, '../models/scaler.pkl')\n",
    "joblib.dump(label_encoders, '../models/label_encoders.pkl')\n",
    "\n",
    "print(\"All data and preprocessing objects saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Our preprocessing pipeline consisted of the following steps:\n",
    "\n",
    "1. Loaded train (100,000 rows) and test (100,000 rows) datasets\n",
    "2. Dropped `other_area` due to high collinearity with `total_area` (r = 0.89)\n",
    "3. Verified no missing values; included median/mode imputation as a safeguard\n",
    "4. Applied IQR-based outlier capping to continuous features (`kitchen_area`, `bath_area`, `extra_area`, `ceil_height`, `total_area`)\n",
    "5. Engineered 4 features: `floor_ratio`, `is_ground_floor`, `is_top_floor`, `living_area`\n",
    "6. Label-encoded `district_name` (7 classes) and `extra_area_type_name` (2 classes)\n",
    "7. Applied StandardScaler to normalise all features\n",
    "8. Split into 80% training and 20% validation sets\n",
    "\n",
    "**Final feature count: 20** (16 original - 1 dropped + 4 engineered + 1 from binary encoding adjustment).\n",
    "\n",
    "The data is now ready for model training in the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
