{
  "nbformat": 4,
  "nbformat_minor": 4,
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.13.2)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 04 - Model Evaluation and Kaggle Submission\n",
        "\n",
        "This notebook evaluates our best-performing model on the held-out validation set and generates predictions for the Kaggle competition submission."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports and Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette('husl')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X_val = np.load('../data/processed/X_val.npy')\n",
        "y_val = np.load('../data/processed/y_val.npy')\n",
        "X_test = np.load('../data/processed/X_test.npy')\n",
        "test_indices = np.load('../data/processed/test_indices.npy')\n",
        "\n",
        "print(f\"Validation set: {X_val.shape}\")\n",
        "print(f\"Test set: {X_test.shape}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Best Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with open('../models/best_model.txt', 'r') as f:\n",
        "    best_model_name = f.read().strip()\n",
        "\n",
        "print(f\"Best model: {best_model_name}\")\n",
        "\n",
        "model_paths = {\n",
        "    'Random Forest': '../models/random_forest.pkl',\n",
        "    'XGBoost': '../models/xgboost.pkl',\n",
        "    'Neural Network': '../models/neural_network.keras'\n",
        "}\n",
        "\n",
        "model_path = model_paths[best_model_name]\n",
        "\n",
        "if best_model_name == 'Neural Network':\n",
        "    from tensorflow import keras\n",
        "    model = keras.models.load_model(model_path)\n",
        "else:\n",
        "    model = joblib.load(model_path)\n",
        "\n",
        "print(f\"Loaded from: {model_path}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Validation Set Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if best_model_name == 'Neural Network':\n",
        "    y_pred = model.predict(X_val, verbose=0).flatten()\n",
        "else:\n",
        "    y_pred = model.predict(X_val)\n",
        "\n",
        "print(f\"Predictions generated for {len(y_pred)} samples\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
        "mae = mean_absolute_error(y_val, y_pred)\n",
        "r2 = r2_score(y_val, y_pred)\n",
        "mape = np.mean(np.abs((y_val - y_pred) / y_val)) * 100\n",
        "\n",
        "print(f\"FINAL MODEL EVALUATION: {best_model_name}\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"RMSE: {rmse:,.2f}\")\n",
        "print(f\"MAE:  {mae:,.2f}\")\n",
        "print(f\"RÂ²:   {r2:.4f}\")\n",
        "print(f\"MAPE: {mape:.2f}%\")\n",
        "print(\"=\" * 50)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Prediction Analysis\n",
        "\n",
        "We compare predicted prices against actual prices and examine the residuals to assess model fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_val, y_pred, alpha=0.5)\n",
        "plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], \n",
        "         'r--', lw=2, label='Perfect Prediction')\n",
        "plt.xlabel('Actual Price')\n",
        "plt.ylabel('Predicted Price')\n",
        "plt.title(f'Actual vs Predicted Prices ({best_model_name})')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "residuals = y_val - y_pred\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "axes[0].scatter(y_pred, residuals, alpha=0.5)\n",
        "axes[0].axhline(y=0, color='r', linestyle='--', lw=2)\n",
        "axes[0].set_xlabel('Predicted Price')\n",
        "axes[0].set_ylabel('Residuals')\n",
        "axes[0].set_title('Residual Plot')\n",
        "\n",
        "axes[1].hist(residuals, bins=50, edgecolor='black')\n",
        "axes[1].set_xlabel('Residuals')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "axes[1].set_title('Distribution of Residuals')\n",
        "axes[1].axvline(x=0, color='r', linestyle='--', lw=2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Residual Statistics:\")\n",
        "print(f\"  Mean: {residuals.mean():,.2f}\")\n",
        "print(f\"  Std:  {residuals.std():,.2f}\")\n",
        "print(f\"  Min:  {residuals.min():,.2f}\")\n",
        "print(f\"  Max:  {residuals.max():,.2f}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if best_model_name in ['Random Forest', 'XGBoost']:\n",
        "    feature_names = open('../data/processed/feature_names.txt').read().split('\\n')\n",
        "    importances = model.feature_importances_\n",
        "\n",
        "    indices = np.argsort(importances)[::-1][:10]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.barh(range(10), importances[indices])\n",
        "    plt.yticks(range(10), [feature_names[i] for i in indices])\n",
        "    plt.xlabel('Feature Importance')\n",
        "    plt.title(f'Top 10 Most Important Features ({best_model_name})')\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nTop 5 Features:\")\n",
        "    for i in range(5):\n",
        "        print(f\"  {i+1}. {feature_names[indices[i]]}: {importances[indices[i]]:.4f}\")\n",
        "else:\n",
        "    print(f\"Feature importance not directly available for {best_model_name}.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Test Set Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if best_model_name == 'Neural Network':\n",
        "    test_predictions = model.predict(X_test, verbose=0).flatten()\n",
        "else:\n",
        "    test_predictions = model.predict(X_test)\n",
        "\n",
        "test_predictions = np.maximum(test_predictions, 0)\n",
        "\n",
        "print(f\"Generated {len(test_predictions)} predictions\")\n",
        "print(f\"\\nPrediction statistics:\")\n",
        "print(f\"  Min:    {test_predictions.min():,.2f}\")\n",
        "print(f\"  Max:    {test_predictions.max():,.2f}\")\n",
        "print(f\"  Mean:   {test_predictions.mean():,.2f}\")\n",
        "print(f\"  Median: {np.median(test_predictions):,.2f}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Kaggle Submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "submission = pd.DataFrame({\n",
        "    'index': test_indices,\n",
        "    'price': test_predictions\n",
        "})\n",
        "\n",
        "print(\"Submission preview:\")\n",
        "print(submission.head(10))\n",
        "print(f\"\\nShape: {submission.shape}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "submission_path = '../submissions/final_submission.csv'\n",
        "submission.to_csv(submission_path, index=False)\n",
        "\n",
        "print(f\"Submission file saved to: {submission_path}\")"
      ],
      "outputs": [],
      "execution_count": null
    }
  ]
}
